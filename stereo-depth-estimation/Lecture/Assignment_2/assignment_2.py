# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/JanNogga/Vision_Systems_Lab/blob/main/Assignment_2/Assignment_2.ipynb
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import random as rand
import numpy as np
import optuna
from optuna.trial import TrialState
from optuna.samplers import TPESampler
import joblib

N_EPOCHS = 180
NUM_STUDY_TRIALS = 6000 # try 6000 configs
STUDY_TIMEOUT = 86400 * 1 # or finish Saturday morning
STUDY_NAME = "CIFAR10_study_3_layer.pkl"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device is : {DEVICE}")

def reject_randomness(manualSeed):
  np.random.seed(manualSeed)
  rand.seed(manualSeed)
  torch.manual_seed(manualSeed)
  if torch.cuda.is_available():
      torch.cuda.manual_seed(manualSeed)
      torch.cuda.manual_seed_all(manualSeed)

  torch.backends.cudnn.enabled = False 
  torch.backends.cudnn.benchmark = False
  torch.backends.cudnn.deterministic = True
  return None

def get_data():
  transform_train = transforms.Compose(
      [transforms.RandomCrop(32, padding=4),
      transforms.RandomHorizontalFlip(),
      transforms.ToTensor(),
      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])
  transform_valid = transforms.Compose(
      [transforms.ToTensor(),
      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])


  trainset = dsets.CIFAR10('data', train=True, download=False, transform=transform_train)
  testset = dsets.CIFAR10('data', train=False, download=False, transform=transform_valid)
  BATCH_SIZE = 4096
  train_loader = torch.utils.data.DataLoader(dataset=trainset, 
                                            batch_size=BATCH_SIZE, 
                                            shuffle=True)
  
  valid_loader = torch.utils.data.DataLoader(dataset=testset, 
                                            batch_size=BATCH_SIZE, 
                                            shuffle=False)
  return train_loader, valid_loader

# network architecture stolen from: https://openreview.net/pdf/1WvovwjA7UMnPB1oinBL.pdf

class GaussianNoise(nn.Module):
    def __init__(self, sigma=0.1, is_relative_detach=True):
        super().__init__()
        self.sigma = sigma
        self.is_relative_detach = is_relative_detach
        self.register_buffer('noise', torch.tensor(0))

    def forward(self, x):
        if self.training and self.sigma != 0:
            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x
            sampled_noise = self.noise.expand(*x.size()).float().normal_() * scale
            x = x + sampled_noise
        return x 

class LogisticRegressionModel(nn.Module):
    def __init__(self, input_dim, z_wide_dim, z_narrow_dim, output_dim, p_drop_in, p_drop_hidden, sigma_in, sigma_hidden, num_z_layers):
        super().__init__()
        self.input_dim = input_dim
        layers = [GaussianNoise(sigma=sigma_in),
                  nn.Dropout(p=p_drop_in),
                  nn.Linear(input_dim, z_wide_dim),
                  GaussianNoise(sigma=sigma_hidden),
                  nn.Dropout(p=p_drop_hidden)]
        for i in range(num_z_layers):
            layers += [nn.Linear(z_wide_dim, z_narrow_dim, bias=False),
                       nn.ReLU(),
                       GaussianNoise(sigma=sigma_hidden),
                       nn.Dropout(p=p_drop_hidden),
                       nn.Linear(z_narrow_dim, z_wide_dim),
                       GaussianNoise(sigma=sigma_hidden),
                       nn.Dropout(p=p_drop_hidden)]
        layers += [nn.Linear(z_wide_dim, output_dim)]
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        x_flat = x.view(-1, self.input_dim)
        out = self.model(x_flat)
        return out


def define_model(trial):

    INPUT_DIM = 3072 # Immutable
    Z_WIDE_DIM = trial.suggest_int('z_wide_dim', 1000, 4000, step=1000) # 1000 - 4000 | 1000 steps - 4 opts
    Z_NARROW_DIM = trial.suggest_int('z_narrow_dim', 100, 1000, step=300) # 100-1000 | 300 steps - 4 opts
    OUTPUT_DIM = 10 # Immutable
    P_DROP_IN = trial.suggest_float("p_drop_in", 0.0, 0.6, step=0.3) # 0.0 - 0.6 | 0.3 steps - 3 opts
    P_DROP_HIDDEN = trial.suggest_float("p_drop_hidden", 0.0, 0.8, step=0.4) # 0.0 - 0.8 | 0.4 steps - 3 opts
    SIGMA_IN = trial.suggest_float("sigma_in", 0.0, 0.5, step=0.25) # 0.0 - 0.5 | 0.25 steps - 3 opts
    SIGMA_HIDDEN = trial.suggest_float("sigma_hidden", 0.0, 1., step=0.5) # 0.0 - 1.0 | 0.5 steps - 3 opts
    NUM_Z_LAYERS = 3

    return LogisticRegressionModel(input_dim=INPUT_DIM, z_wide_dim=Z_WIDE_DIM, z_narrow_dim=Z_NARROW_DIM, output_dim=OUTPUT_DIM, p_drop_in=P_DROP_IN, p_drop_hidden=P_DROP_HIDDEN, sigma_in=SIGMA_IN, sigma_hidden=SIGMA_HIDDEN, num_z_layers=NUM_Z_LAYERS)

criterion = nn.CrossEntropyLoss().to(DEVICE)

def objective(trial):
  reject_randomness(2021)
  model = define_model(trial).to(DEVICE)
  with torch.no_grad():
    for param in model.parameters():
        param.data = param.data - param.data.mean()

  LEARNING_RATE = trial.suggest_float("learning_rate", 1e-4, 1e-1, log=True) # 1e-1, 1e-2, 1e-3, 1e-4 - 4 opts
  WEIGHT_DECAY = trial.suggest_float("weight_decay", 1e-4, 1, log=True) # 1, 1e-2, 1e-4, 0 - 4 opts
  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

  train_loader, valid_loader = get_data()

  for epoch in range(N_EPOCHS):
    model.train()
    for imgs, labels in train_loader:
      optimizer.zero_grad()
      labels = labels.to(DEVICE)
      pred_labels = model(imgs.to(DEVICE))
      loss = criterion(pred_labels, labels)
  
      loss.backward()
      optimizer.step()

    model.eval()
    batch_sizes_valid, valid_acc_list = [], []
    for imgs, labels in valid_loader:
      with torch.no_grad():
        labels = labels.to(DEVICE)
        pred_labels = model(imgs.to(DEVICE))
        batch_sizes_valid.append(imgs.shape[0])
        pred_label_inds = pred_labels.argmax(dim=-1)
        acc = (pred_label_inds == labels).float().mean() * 100
        valid_acc_list.append(acc.item())

    batch_sizes_valid = np.array(batch_sizes_valid)
    valid_acc_list = np.array(valid_acc_list)
    weighted_valid_acc_list = valid_acc_list*batch_sizes_valid
    accuracy = weighted_valid_acc_list.sum()/batch_sizes_valid.sum()

    trial.report(accuracy, epoch)

    if trial.should_prune():
        raise optuna.exceptions.TrialPruned()
        
  joblib.dump(trial.study, STUDY_NAME)
  return accuracy

# sampler=TPESampler(n_startup_trials=20,multivariate=True)
study = optuna.create_study(study_name="CIFAR10_trials_3_layer", direction="maximize", sampler=TPESampler(n_startup_trials=5,multivariate=True))
study.optimize(objective, n_trials=NUM_STUDY_TRIALS, timeout=STUDY_TIMEOUT)

pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

print("Study statistics: ")
print("  Number of finished trials: ", len(study.trials))
print("  Number of pruned trials: ", len(pruned_trials))
print("  Number of complete trials: ", len(complete_trials))

print("Best trial:")
trial = study.best_trial

print("  Value: ", trial.value)

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))
