{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b722bd6d",
   "metadata": {},
   "source": [
    "### Pretraining Script Prototyping\n",
    "#### Import Statements & WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB setup first\n",
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import warnings\n",
    "from asset.utils import *\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "from asset.transforms import DisparityTransformCompose, DisparityTransform, RandomCrop, Interpolate\n",
    "from asset.datasets import DisparityDataset\n",
    "from asset.criterion import NonZeroWrapper, IntermediateSupervisionWrapper, PE\n",
    "from asset.model import BaselineModel, CustomModel\n",
    "from asset.trainer import Trainer\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "            \n",
    "#os.environ['WANDB_MODE'] = 'dryrun' #uncomment to prevent wandb logging - 'offline mode'\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    encoder_depth= 4, #3\n",
    "    encoder_backbone='efficientnet-b3', #mobilenet_v2\n",
    "    height_warmup=512,\n",
    "    width_warmup=928,\n",
    "    height=528,\n",
    "    width=944,\n",
    "    n_epochs_warmup=2, #5\n",
    "    n_epochs=2,\n",
    "    batch_size_train_warmup=8,\n",
    "    batch_size_valid_warmup=32,\n",
    "    batch_size_train=2,\n",
    "    batch_size_valid=8,\n",
    "    lr=1e-3,\n",
    "    lr_decay_warmup=0.7,\n",
    "    lr_decay=0.85\n",
    ")\n",
    "\n",
    "wandb.init(config=hyperparameter_defaults, project=\"CudaLab_SS21_Pretraining\")\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Cuda Version:\", torch.version.cuda)\n",
    "print(\"CUDNN Version:\", torch.backends.cudnn.version())\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "experiment_title = config.encoder_backbone + f'_encoder_depth_{config.encoder_depth}'+'_pretrained'\n",
    "print('Experiment title:', experiment_title)\n",
    "pretty_print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3eeb53",
   "metadata": {},
   "source": [
    "#### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bac56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_warmup = DisparityTransformCompose([DisparityTransform(transforms.Normalize(mean_imagenet, std_imagenet), apply_to_disparity=False),\n",
    "                                              RandomCrop(output_size=(config.height_warmup,config.width_warmup)),\n",
    "                                              Interpolate(scale_factor=0.5)])\n",
    "\n",
    "transform = DisparityTransformCompose([DisparityTransform(transforms.Normalize(mean_imagenet, std_imagenet), apply_to_disparity=False),\n",
    "                                       RandomCrop(output_size=(config.height,config.width))])\n",
    "\n",
    "monkaa_dataset_warmup = DisparityDataset(get_left_frames_monkaa(), left_to_right_monkaa, left_to_disparity_monkaa, load_image_sceneflow, load_disparity_sceneflow, transform_warmup)\n",
    "driving_dataset_warmup = DisparityDataset(get_left_frames_driving(), left_to_right_driving, left_to_disparity_driving, load_image_sceneflow, load_disparity_sceneflow, transform_warmup)\n",
    "flyingthings3d_dataset_warmup = DisparityDataset(get_left_frames_flyingthings3d(use_difficult_examples=False), left_to_right_flyingthings3d, left_to_disparity_flyingthings3d, load_image_sceneflow, load_disparity_sceneflow, transform_warmup)\n",
    "concat_dataset_warmup = ConcatDataset([monkaa_dataset_warmup, driving_dataset_warmup, flyingthings3d_dataset_warmup])\n",
    "\n",
    "\n",
    "monkaa_dataset = DisparityDataset(get_left_frames_monkaa(), left_to_right_monkaa, left_to_disparity_monkaa, load_image_sceneflow, load_disparity_sceneflow, transform)\n",
    "driving_dataset = DisparityDataset(get_left_frames_driving(), left_to_right_driving, left_to_disparity_driving, load_image_sceneflow, load_disparity_sceneflow, transform)\n",
    "flyingthings3d_dataset = DisparityDataset(get_left_frames_flyingthings3d(use_difficult_examples=False), left_to_right_flyingthings3d, left_to_disparity_flyingthings3d, load_image_sceneflow, load_disparity_sceneflow, transform)\n",
    "print('Number of samples in the Monkaa dataset:', len(monkaa_dataset))\n",
    "print('Number of samples in the Driving dataset:', len(driving_dataset))\n",
    "print('Number of samples in the Flyingthings3d dataset:', len(flyingthings3d_dataset))\n",
    "concat_dataset = ConcatDataset([monkaa_dataset, driving_dataset, flyingthings3d_dataset])\n",
    "print('Number of samples in the concatenated Sceneflow dataset:', len(concat_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c5255",
   "metadata": {},
   "source": [
    "#### Show Warmup Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b867a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand_idx = np.random.choice(np.arange(len(concat_dataset_warmup)), size=3, replace=False)\n",
    "rand_idx = [1871, 12612, 31972]\n",
    "for i in rand_idx:\n",
    "    (img_left, img_right), img_disparity = concat_dataset_warmup[i]\n",
    "    \n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    show_image(inv_normalize(img_left), ax=axes[0], title='left image')\n",
    "    \n",
    "    show_image(inv_normalize(img_right), ax=axes[1], title='right image')\n",
    "    \n",
    "    show_disparity(img_disparity, ax=axes[2], title='disparity map')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a6b2d",
   "metadata": {},
   "source": [
    "#### Show Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99c3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in rand_idx:\n",
    "    (img_left, img_right), img_disparity = concat_dataset[i]\n",
    "    \n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    show_image(inv_normalize(img_left), ax=axes[0], title='left image')\n",
    "    \n",
    "    show_image(inv_normalize(img_right), ax=axes[1], title='right image')\n",
    "    \n",
    "    show_disparity(img_disparity, ax=axes[2], title='disparity map')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ef92e",
   "metadata": {},
   "source": [
    "#### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba240ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share_concat = 0.8\n",
    "N_samples_concat = len(concat_dataset)\n",
    "N_samples_concat = 400\n",
    "indices_concat = list(range(N_samples_concat))\n",
    "split_concat = int(np.ceil(train_share_concat * N_samples_concat))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices_concat)\n",
    "\n",
    "BATCHSIZE_CONCAT_WARMUP = config.batch_size_train_warmup\n",
    "BATCHSIZE_CONCAT_VALID_WARMUP = config.batch_size_valid_warmup\n",
    "\n",
    "BATCHSIZE_CONCAT = config.batch_size_train\n",
    "BATCHSIZE_CONCAT_VALID = config.batch_size_valid\n",
    "\n",
    "train_idx_concat, valid_idx_concat = indices_concat[:split_concat], indices_concat[split_concat:]\n",
    "print('Number of training : validation samples -', len(train_idx_concat), ':', len(valid_idx_concat))\n",
    "\n",
    "trainset_concat_warmup = torch.utils.data.Subset(concat_dataset_warmup, train_idx_concat)\n",
    "validset_concat_warmup = torch.utils.data.Subset(concat_dataset_warmup, valid_idx_concat)\n",
    "\n",
    "trainset_concat = torch.utils.data.Subset(concat_dataset, train_idx_concat)\n",
    "validset_concat = torch.utils.data.Subset(concat_dataset, valid_idx_concat)\n",
    "\n",
    "trainloader_concat_warmup = torch.utils.data.DataLoader(trainset_concat_warmup, batch_size=BATCHSIZE_CONCAT_WARMUP, shuffle=True, num_workers=10)\n",
    "validloader_concat_warmup = torch.utils.data.DataLoader(validset_concat_warmup, batch_size=BATCHSIZE_CONCAT_VALID_WARMUP, shuffle=False, num_workers=10)\n",
    "\n",
    "trainloader_concat = torch.utils.data.DataLoader(trainset_concat, batch_size=BATCHSIZE_CONCAT, shuffle=True, num_workers=10)\n",
    "validloader_concat = torch.utils.data.DataLoader(validset_concat, batch_size=BATCHSIZE_CONCAT_VALID, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ddd8a",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ed1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_smoothl1 = torch.nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "wrapped_criterion_smoothl1 = NonZeroWrapper(torch.nn.SmoothL1Loss(reduction='none', beta=1.0), max_disp=192)\n",
    "wrapped_criterion_smoothl1 = IntermediateSupervisionWrapper(wrapped_criterion_smoothl1)\n",
    "wrapped_criterion_1PE = NonZeroWrapper(PE(reduction='none', threshold=1))\n",
    "wrapped_criterion_3PE = NonZeroWrapper(PE(reduction='none'))\n",
    "wrapped_criterion_5PE = NonZeroWrapper(PE(reduction='none', threshold=5))\n",
    "metrics = {\n",
    "  \"1PE\": wrapped_criterion_1PE,\n",
    "  \"3PE\": wrapped_criterion_3PE,\n",
    "  \"5PE\": wrapped_criterion_5PE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afb0e0",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = CustomModel(encoder_depth=config.encoder_depth, backbone=config.encoder_backbone)\n",
    "\n",
    "n_params = count_parameters(model_custom)\n",
    "wandb.run.summary['Parameter Number'] = n_params\n",
    "print(f'The model has {n_params} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522111f",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = experiment_title\n",
    "custom_trainer = Trainer(model_custom, wrapped_criterion_smoothl1, trainloader_concat_warmup, validloader_concat_warmup, eval_metrics=metrics, es_mode='min', description=title, patience=32, lr=config.lr, n_epochs=config.n_epochs_warmup, lr_decay=config.lr_decay_warmup)\n",
    "custom_trainer.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ca161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = custom_trainer.model\n",
    "custom_trainer = Trainer(model_custom, wrapped_criterion_smoothl1, trainloader_concat, validloader_concat, eval_metrics=metrics, es_mode='min', description=title, patience=32, lr=config.lr, n_epochs=config.n_epochs, lr_decay=config.lr_decay)\n",
    "custom_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96787a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save('models/'+experiment_title+'_best.pt')\n",
    "wandb.save('trainer_logs/'+experiment_title+'.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
