{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b722bd6d",
   "metadata": {},
   "source": [
    "### Training Script Prototyping\n",
    "#### Import Statements & WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB setup first\n",
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import warnings\n",
    "from asset.utils import *\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "from asset.transforms import DisparityTransformCompose, DisparityTransform, RandomCrop, Interpolate, CutMix\n",
    "from asset.datasets import DisparityDataset\n",
    "from asset.criterion import NonZeroWrapper, IntermediateSupervisionWrapper, PE\n",
    "from asset.model import BaselineModel, CustomModel\n",
    "from asset.trainer import TrainerKitti\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "            \n",
    "#os.environ['WANDB_MODE'] = 'dryrun' #uncomment to prevent wandb logging - 'offline mode'\n",
    "\n",
    "hyperparameter_defaults = dict(\n",
    "    encoder_depth= 4,\n",
    "    encoder_backbone='efficientnet-b3', #mobilenet_v2\n",
    "    height_train=256,\n",
    "    width_train=512,\n",
    "    height_valid=368,\n",
    "    width_valid=1248,\n",
    "    n_epochs=1000,\n",
    "    patience=12,\n",
    "    batch_size_train=8,\n",
    "    batch_size_valid=1,\n",
    "    lr=1e-3,\n",
    "    lr_decay=0.992,\n",
    "    pretrained=True,\n",
    "    augment_data=True\n",
    ")\n",
    "\n",
    "wandb.init(config=hyperparameter_defaults, project=\"CudaLab_SS21\")\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"Cuda Version:\", torch.version.cuda)\n",
    "print(\"CUDNN Version:\", torch.backends.cudnn.version())\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "pretrained_title = config.encoder_backbone + f'_encoder_depth_{config.encoder_depth}'+'_pretrained'\n",
    "pretrain_str = '_pretrain' if config.pretrained else '_nopretrain'\n",
    "augment_str = '_augment' if config.augment_data else '_noaugment'\n",
    "experiment_title = config.encoder_backbone + pretrain_str + augment_str\n",
    "print('Experiment title:', experiment_title)\n",
    "pretty_print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3eeb53",
   "metadata": {},
   "source": [
    "#### Dataset and Dataloader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bac56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN, STD = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = DisparityTransformCompose([DisparityTransform(transforms.Normalize(MEAN, STD), apply_to_disparity=False),\n",
    "                                       RandomCrop(output_size=(config.height_train,config.width_train))])\n",
    "\n",
    "\n",
    "# transform_large = DisparityTransformCompose([DisparityTransform(transforms.Normalize(MEAN, STD), apply_to_disparity=False),\n",
    "#                                        RandomCrop(output_size=(352,1216))])\n",
    "\n",
    "transform_large = DisparityTransformCompose([DisparityTransform(transforms.Normalize(MEAN, STD), apply_to_disparity=False)])\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(MEAN, STD)],\n",
    "   std= [1/s for s in STD]\n",
    ")\n",
    "\n",
    "\n",
    "kitti_dataset = DisparityDataset(get_left_frames_kitti(), left_to_right_kitti, left_to_disparity_kitti, load_image_kitti, load_disparity_kitti, transform)\n",
    "kitti_dataset_large = DisparityDataset(get_left_frames_kitti(), left_to_right_kitti, left_to_disparity_kitti, load_image_kitti, load_disparity_kitti, transform_large)\n",
    "print('Number of samples in the KITTI dataset:', len(kitti_dataset))\n",
    "\n",
    "train_share = 0.75\n",
    "N_samples = len(kitti_dataset)\n",
    "#N_samples = 40 ### REMOVE THIS LINE AFTER DEBUGGING!!!\n",
    "indices = list(range(N_samples))\n",
    "split = int(np.ceil(train_share * N_samples))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[:split], indices[split:]\n",
    "print('Number of training : validation samples -', len(train_idx), ':', len(valid_idx))\n",
    "trainset = torch.utils.data.Subset(kitti_dataset, train_idx)\n",
    "validset = torch.utils.data.Subset(kitti_dataset_large, valid_idx)\n",
    "\n",
    "if config.augment_data:\n",
    "    transform = DisparityTransformCompose([transform,\n",
    "                                       CutMix(trainset)])\n",
    "    kitti_dataset = DisparityDataset(get_left_frames_kitti(), left_to_right_kitti, left_to_disparity_kitti, load_image_kitti, load_disparity_kitti, transform)\n",
    "    trainset = torch.utils.data.Subset(kitti_dataset, train_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size_train, shuffle=True, num_workers=10)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=config.batch_size_valid, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a6b2d",
   "metadata": {},
   "source": [
    "#### Show Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99c3c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_idx = np.random.choice(np.arange(len(trainset)), size=3, replace=False)\n",
    "#rand_idx = [1871, 12612, 31972]\n",
    "for i in rand_idx:\n",
    "    (img_left, img_right), img_disparity = trainset[i]\n",
    "    \n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    show_image(inv_normalize(img_left), ax=axes[0], title='left image')\n",
    "    \n",
    "    show_image(inv_normalize(img_right), ax=axes[1], title='right image')\n",
    "    \n",
    "    show_disparity(img_disparity, ax=axes[2], title='disparity map')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ddd8a",
   "metadata": {},
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ed1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_smoothl1 = torch.nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "wrapped_criterion_smoothl1 = NonZeroWrapper(torch.nn.SmoothL1Loss(reduction='none', beta=1.0), max_disp=192)\n",
    "wrapped_criterion_smoothl1 = IntermediateSupervisionWrapper(wrapped_criterion_smoothl1)\n",
    "wrapped_criterion_1PE = NonZeroWrapper(PE(reduction='none', threshold=1))\n",
    "wrapped_criterion_3PE = NonZeroWrapper(PE(reduction='none'))\n",
    "wrapped_criterion_5PE = NonZeroWrapper(PE(reduction='none', threshold=5))\n",
    "metrics = {\n",
    "  \"1PE\": wrapped_criterion_1PE,\n",
    "  \"3PE\": wrapped_criterion_3PE,\n",
    "  \"5PE\": wrapped_criterion_5PE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afb0e0",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = CustomModel(encoder_depth=config.encoder_depth, backbone=config.encoder_backbone)\n",
    "\n",
    "n_params = count_parameters(model_custom)\n",
    "wandb.run.summary['Parameter Number'] = n_params\n",
    "print(f'The model has {n_params} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522111f",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pretrained:\n",
    "    print('Loading pretrained model!')\n",
    "    title = pretrained_title\n",
    "    pretrain_loader = TrainerKitti(model_custom, wrapped_criterion_smoothl1, trainloader, validloader, eval_metrics=metrics, es_mode='min', description=title, patience=32, lr=config.lr, n_epochs=config.n_epochs, lr_decay=config.lr_decay, size_y=config.height_valid, size_x=config.width_valid)\n",
    "    pretrain_loader.load_model()\n",
    "    model_custom = pretrain_loader.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = experiment_title\n",
    "custom_trainer = TrainerKitti(model_custom, wrapped_criterion_smoothl1, trainloader, validloader, eval_metrics=metrics, es_mode='min', description=title, patience=config.patience, lr=config.lr, n_epochs=config.n_epochs, lr_decay=config.lr_decay, size_y=config.height_valid, size_x=config.width_valid)\n",
    "custom_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96787a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.save('models/'+experiment_title+'_best.pt')\n",
    "wandb.save('trainer_logs/'+experiment_title+'.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
