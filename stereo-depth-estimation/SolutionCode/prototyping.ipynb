{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prototyping.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP+O63Awib1qIp0L5FMhpVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanNogga/Vision_Systems_Lab/blob/main/Project_Solution/prototyping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szjX9ntMkzQi"
      },
      "source": [
        "!pip install segmentation-models-pytorch >> /dev/null\n",
        "!pip install cmocean >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyCutrWlY8W7"
      },
      "source": [
        "import numpy as np\n",
        "import random as rand\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "import random\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.jit as jit\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW_6haPhqW8E"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIayjgL_ZuDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ac9587-9089-473d-981e-7e84760e8ff6"
      },
      "source": [
        "B, C, H, W = 1, 3, 640, 480 #368, 1216\n",
        "batch_a = torch.randn(B, C, H, W).cuda()\n",
        "batch_b = torch.randn(B, C, H, W).cuda()\n",
        "print(batch_a.shape, batch_b.shape)\n",
        "print(torch.allclose(batch_a, batch_b), torch.allclose(batch_a, batch_a), torch.allclose(batch_b, batch_b))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 640, 480]) torch.Size([1, 3, 640, 480])\n",
            "False True True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h0uPMcUI-Il"
      },
      "source": [
        "# Here we are adapting the source code of the Unet from https://github.com/qubvel/segmentation_models.pytorch\n",
        "# to fit our Feature Fusion needs. Specifically, the Unet building blocks are inspired by the ones at: https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/unet/decoder.py\n",
        "# The reason we adapt this code right here instead of writing it from scratch is because the pretrained segmentation backbones available at the repository above are then completely compatible with our Feature Fusion.\n",
        "# This yields a huge variety of possible encoder backbones that we could use.\n",
        "\n",
        "from segmentation_models_pytorch.base import modules as md\n",
        "\n",
        "class CustomDecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            skip_channels,\n",
        "            out_channels,\n",
        "            use_batchnorm=True,\n",
        "            attention_type=None,\n",
        "            upsample=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2) if upsample else nn.Identity()\n",
        "        self.conv1 = md.Conv2dReLU(\n",
        "            in_channels + skip_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.attention1 = md.Attention(attention_type, in_channels=in_channels + skip_channels)\n",
        "        self.conv2 = md.Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = self.upsample(x)\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "            x = self.attention1(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.attention2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CustomCenterBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
        "        conv1 = md.Conv2dReLU(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        conv2 = md.Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        super().__init__(conv1, conv2)\n",
        "\n",
        "\n",
        "class CustomUnetDecoder(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            encoder_channels,\n",
        "            decoder_channels,\n",
        "            use_batchnorm=True,\n",
        "            attention_type=None,\n",
        "            center=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        n_blocks = len(decoder_channels)\n",
        "\n",
        "        encoder_channels = encoder_channels[:n_blocks+1]\n",
        "        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n",
        "        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n",
        "        upsample = [True for channel in encoder_channels]\n",
        "        upsample[-2:] = [False, False]\n",
        "        self.num_downsample = [0 for channel in encoder_channels]\n",
        "        self.num_downsample[-2:] = [1, 2]\n",
        "        # computing blocks input and output channels\n",
        "        head_channels = encoder_channels[0]\n",
        "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "\n",
        "        if center:\n",
        "            self.center = CustomCenterBlock(\n",
        "                head_channels, head_channels, use_batchnorm=use_batchnorm\n",
        "            )\n",
        "        else:\n",
        "            self.center = nn.Identity()\n",
        "\n",
        "        # combine decoder keyword arguments\n",
        "        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n",
        "        blocks = [\n",
        "            CustomDecoderBlock(in_ch, skip_ch, out_ch, upsample=ups, **kwargs)\n",
        "            for in_ch, skip_ch, out_ch, ups in zip(in_channels, skip_channels, out_channels, upsample)\n",
        "        ]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.downsample = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    def forward(self, *features):\n",
        "\n",
        "        features = features[1:]    # remove first skip with same spatial resolution\n",
        "        features = features[::-1]  # reverse channels to start from head of encoder\n",
        "\n",
        "        head = features[0]\n",
        "        skips = features[1:]\n",
        "        feature_maps = []\n",
        "        x = self.center(head)\n",
        "        for i, decoder_block in enumerate(self.blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            if skip is not None:\n",
        "                for j in range(self.num_downsample[i]):\n",
        "                    skip = self.downsample(skip)\n",
        "            x = decoder_block(x, skip)\n",
        "            feature_maps.append(x)\n",
        "        x = feature_maps[0]\n",
        "        feature_maps = feature_maps[1:]\n",
        "        for i in range(len(feature_maps)-2):\n",
        "                x = torch.cat([self.upsample(x), feature_maps[i]], dim=1)\n",
        "        for i in range(2):\n",
        "                x = torch.cat([x, feature_maps[-(i+1)]], dim=1)\n",
        "        return x\n",
        "\n",
        "class UnetFeatures(nn.Module):\n",
        "    def __init__(self, encoder_depth=3, decoder_channels=(64, 32, 16, 12, 8), backbone='resnet34'):\n",
        "        super().__init__()\n",
        "        model = smp.Unet(encoder_name=backbone, encoder_weights=\"imagenet\", encoder_depth=encoder_depth, decoder_channels=decoder_channels[:encoder_depth]) # \"efficientnet-b3\"\n",
        "        self.encoder = model.encoder\n",
        "        self.decoder = CustomUnetDecoder(encoder_channels=model.encoder._out_channels, decoder_channels=decoder_channels[:encoder_depth], center=True)\n",
        "        self._freeze_encoder()\n",
        "\n",
        "    def _freeze_encoder(self):\n",
        "        self.encoder.requires_grad_(False)\n",
        "  \n",
        "    def _unfreeze_encoder(self):\n",
        "        self.encoder.requires_grad_(True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_features = self.encoder(x)\n",
        "        return self.decoder(*enc_features)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cZET6ChM6lA"
      },
      "source": [
        "# ed = 5\n",
        "# tmp = smp.Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", encoder_depth=ed, decoder_channels=(64, 32, 16, 12, 8)[:ed]).cuda() # \"efficientnet-b3\", \"mobilenet_v2\", \"resnet34\"\n",
        "# tmp_feats = tmp.encoder(batch_a)\n",
        "# for map in tmp_feats:\n",
        "#   print(map.shape)\n",
        "\n",
        "# print(tmp.encoder._out_channels[:2])\n",
        "# decoder = CustomUnetDecoder(encoder_channels=tmp.encoder._out_channels, decoder_channels=(64, 32, 16, 12, 8)[:ed], center=True).cuda()\n",
        "# #print(decoder)\n",
        "# out = decoder(*tmp_feats)\n",
        "# print(out.shape)\n",
        "# print(sum((64, 32, 16, 12, 8)[:ed]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIUDs4s-F-U"
      },
      "source": [
        "class StackedHourglass(nn.Module):\n",
        "    def __init__(self, inplanes, norm_layer = None):\n",
        "        super(StackedHourglass, self).__init__()\n",
        "        self.C = inplanes\n",
        "\n",
        "        self.layer_1_2 = nn.Sequential(nn.Conv3d(self.C, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C//2),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Conv3d(self.C//2, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C//2),\n",
        "                                       nn.ReLU(inplace=True))\n",
        "        self.layer_2_4 = nn.Sequential(nn.Conv3d(self.C//2, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C//2),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Conv3d(self.C//2, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C//2))\n",
        "        self.layer_4_6 = nn.Sequential(nn.Conv3d(self.C//2, self.C, kernel_size=3, padding=1, stride=2, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C),\n",
        "                                       nn.ReLU(inplace=True))\n",
        "        self.layer_6_8 = nn.Sequential(nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, stride=2, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, bias=False),\n",
        "                                       nn.BatchNorm3d(self.C),\n",
        "                                       nn.ReLU(inplace=True))\n",
        "        self.layer_8_9 = nn.Sequential(nn.ConvTranspose3d(self.C, self.C, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n",
        "                                       nn.BatchNorm3d(self.C))\n",
        "        self.layer_9_10 = nn.Sequential(nn.ConvTranspose3d(self.C, self.C//2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n",
        "                                        nn.BatchNorm3d(self.C//2))\n",
        "        self.layer_10_12 = nn.Sequential(nn.Conv3d(self.C//2, self.C, kernel_size=3, padding=1, stride=2, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C),\n",
        "                                         nn.ReLU(inplace=True),\n",
        "                                         nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C))\n",
        "        self.layer_12_14 = nn.Sequential(nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, stride=2, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C),\n",
        "                                         nn.ReLU(inplace=True),\n",
        "                                         nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C),\n",
        "                                         nn.ReLU(inplace=True))\n",
        "        self.layer_14_15 = nn.Sequential(nn.ConvTranspose3d(self.C, self.C, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n",
        "                                         nn.BatchNorm3d(self.C))\n",
        "        self.layer_15_16 = nn.Sequential(nn.ConvTranspose3d(self.C, self.C//2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n",
        "                                         nn.BatchNorm3d(self.C//2)) \n",
        "        self.layer_16_18 = nn.Sequential(nn.Conv3d(self.C//2, self.C, kernel_size=3, padding=1, stride=2, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C),\n",
        "                                         nn.ReLU(inplace=True),\n",
        "                                         nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C))\n",
        "        self.layer_18_20 = nn.Sequential(nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, stride=2, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C),\n",
        "                                         nn.ReLU(inplace=True),\n",
        "                                         nn.Conv3d(self.C, self.C, kernel_size=3, padding=1, bias=False),\n",
        "                                         nn.BatchNorm3d(self.C),\n",
        "                                         nn.ReLU(inplace=True))\n",
        "        self.layer_20_21 = nn.Sequential(nn.ConvTranspose3d(self.C, self.C, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n",
        "                                         nn.BatchNorm3d(self.C))\n",
        "        self.layer_21_22 = nn.Sequential(nn.ConvTranspose3d(self.C, self.C//2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n",
        "                                         nn.BatchNorm3d(self.C//2))\n",
        "      \n",
        "        self.dist1 = nn.Sequential(nn.Conv3d(self.C//2, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                   nn.BatchNorm3d(self.C//2),\n",
        "                                   nn.ReLU(inplace=True),\n",
        "                                   nn.Conv3d(self.C//2, 1, kernel_size=3, padding=1,bias=False))\n",
        "        self.dist2 = nn.Sequential(nn.Conv3d(self.C//2, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                   nn.BatchNorm3d(self.C//2),\n",
        "                                   nn.ReLU(inplace=True),\n",
        "                                   nn.Conv3d(self.C//2, 1, kernel_size=3, padding=1,bias=False))\n",
        "        self.dist3 = nn.Sequential(nn.Conv3d(self.C//2, self.C//2, kernel_size=3, padding=1, bias=False),\n",
        "                                   nn.BatchNorm3d(self.C//2),\n",
        "                                   nn.ReLU(inplace=True),\n",
        "                                   nn.Conv3d(self.C//2, 1, kernel_size=3, padding=1,bias=False))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # 2x conv3d \n",
        "        a2 = self.layer_1_2(x) #-- 64->32\n",
        "        # 2x conv3d\n",
        "        a4 = F.relu(self.layer_2_4(a2) + a2, inplace=True) #-- 32->32\n",
        "        ## hourglass 1 \n",
        "        # 1x downconv + 1x conv\n",
        "        a6 = self.layer_4_6(a4) #-- 32->64\n",
        "        # 1x downconv + 1x conv\n",
        "        a8 = self.layer_6_8(a6) #-- 64->64\n",
        "        # 2x convtranspose\n",
        "        a9 = F.relu(self.layer_8_9(a8)+ a6, inplace=True) #-- 64->64\n",
        "        a10 = F.relu(self.layer_9_10(a9)+ a4, inplace=True) #-- 64->32\n",
        "        ## hourglass 2\n",
        "        # 1x downconv + 1x conv\n",
        "        a12 = F.relu(self.layer_10_12(a10) + a9, inplace=True) #-- 32->64\n",
        "        # 1x downconv + 1x conv\n",
        "        a14 = self.layer_12_14(a12) #-- 64->64\n",
        "        # 2x convtranspose\n",
        "        a15 = F.relu(self.layer_14_15(a14)+ a6, inplace=True) #-- 64->64\n",
        "        a16 = F.relu(self.layer_15_16(a15)+ a4, inplace=True) #-- 64->32\n",
        "        ## hourglass 3\n",
        "        # 1x downconv + 1x conv\n",
        "        a18 = F.relu(self.layer_16_18(a16) + a15, inplace=True) #-- 32->64\n",
        "        # 1x downconv + 1x conv\n",
        "        a20 = self.layer_18_20(a18) #-- 64->64\n",
        "        # 2x convtranspose\n",
        "        a21 = F.relu(self.layer_20_21(a20)+ a6, inplace=True) #-- 64->64\n",
        "        a22 = F.relu(self.layer_21_22(a21)+ a4, inplace=True) #-- 64->32\n",
        "\n",
        "        c1 = self.dist1(a10) #-- 32->1\n",
        "        c2 = self.dist2(a16) + c1 #-- 32->1\n",
        "        c3 = self.dist2(a22) + c2 #-- 32->1\n",
        "        if self.training:\n",
        "            return (c1, c2, c3)\n",
        "        else:\n",
        "            return c3"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0wH0lOf9D1V"
      },
      "source": [
        "# squeeze layer\n",
        "class Squeeze(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(Squeeze, self).__init__()\n",
        "        self.dim = dim\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # type: (Tensor) -> Tensor\n",
        "        return x.squeeze(self.dim)\n",
        "\n",
        "\n",
        "# disparity expectation layer\n",
        "class Expectation(nn.Module):\n",
        "    def __init__(self, max_disp):\n",
        "        super(Expectation, self).__init__()\n",
        "        # this is a parameter without grad to ensure model.cuda() behaves as expected\n",
        "        self.weights = nn.parameter.Parameter(torch.arange(max_disp).float(), requires_grad=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # type: (Tensor) -> Tensor\n",
        "        return (x.permute(0,2,3,1) @ self.weights).unsqueeze(1)\n",
        "\n",
        "\n",
        "# group-wise corr as per GWC net: https://github.com/xy-guo/GwcNet/blob/master/models/submodule.py\n",
        "class GWC(nn.Module):\n",
        "    def __init__(self, max_disp, num_groups):\n",
        "        super(GWC, self).__init__()\n",
        "        self.max_disp = max_disp\n",
        "        self.num_groups = num_groups\n",
        "    \n",
        "    def forward(self, left_feats, right_feats):\n",
        "        # type: (Tensor, Tensor) -> Tensor\n",
        "        B, C, H_prime, W_prime = left_feats.shape\n",
        "        channels_per_group = C // self.num_groups\n",
        "        cost = (left_feats * right_feats).view([B, self.num_groups, channels_per_group, H_prime, W_prime]).mean(dim=2)\n",
        "        return cost\n",
        "\n",
        "# cost as per GWC net: https://github.com/xy-guo/GwcNet/blob/master/models/submodule.py\n",
        "class GWCBuilder(nn.Module):\n",
        "    def __init__(self, max_disp, num_groups):\n",
        "        super(GWCBuilder, self).__init__()\n",
        "        self.gwc = GWC(max_disp, num_groups)\n",
        "        self.max_disp = max_disp\n",
        "        self.num_groups = num_groups\n",
        "    \n",
        "    def forward(self, left_feats, right_feats):\n",
        "        # type: (Tensor, Tensor) -> Tensor\n",
        "        B, C, H_prime, W_prime = left_feats.shape\n",
        "        cost = torch.zeros(B, self.num_groups, self.max_disp//4, H_prime, W_prime).to(left_feats.device)\n",
        "        for i in range(self.max_disp//4):\n",
        "            if i > 0:\n",
        "                cost[:, :, i, :, i:] = self.gwc(left_feats[:, :, :, i:], right_feats[:, :, :, :-i])\n",
        "            else:\n",
        "                cost[:, :, i, :, :] = self.gwc(left_feats, right_feats)\n",
        "        return cost\n",
        "\n",
        "\n",
        "# cost volume construction is taken from the lab slides\n",
        "class CostBuilder(nn.Module):\n",
        "    def __init__(self, max_disp):\n",
        "        super(CostBuilder, self).__init__()\n",
        "        self.max_disp = max_disp\n",
        "    \n",
        "    def forward(self, left_feats, right_feats):\n",
        "        # type: (Tensor, Tensor) -> Tensor\n",
        "        B, C, H_prime, W_prime = left_feats.shape\n",
        "        cost = torch.zeros(B, 2*C, self.max_disp//4, H_prime, W_prime).to(left_feats.device)\n",
        "        for i in range(self.max_disp//4):\n",
        "            if i==0:\n",
        "                cost[:,:C,i,:,:] = left_feats\n",
        "                cost[:,C:,i,:,:] = right_feats\n",
        "            else:\n",
        "                cost[:,:C,i,:,i:] = left_feats[:,:,:,i:]\n",
        "                cost[:,C:,i,:,i:] = right_feats[:,:,:,:-i]\n",
        "        return cost\n",
        "\n",
        "# residual blocks are built like shown in https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride = 1, norm_layer = None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = None if (stride == 1 and inplanes == planes) else nn.Sequential(conv1x1(inplanes, planes, stride), norm_layer(planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # type: (Tensor) -> Tensor\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "def conv3d3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv3d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv3d1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv3d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "    \n",
        "class BasicBlock3D(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride = 1, norm_layer = None):\n",
        "        super(BasicBlock3D, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm3d\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3d3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3d3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = None if (stride == 1 and inplanes == planes) else nn.Sequential(conv3d1x1(inplanes, planes, stride), norm_layer(planes))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # type: (Tensor) -> Tensor\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, in_channels=3, max_disp=192, encoder_depth=3, decoder_channels=(256, 128, 64, 32, 16), backbone='resnet34'): #136\n",
        "        super(CustomModel, self).__init__()\n",
        "        # features:\n",
        "        # in:  B  x  3 x H    x W\n",
        "        # out: B x  C x H//4 x W//4\n",
        "        self.features = UnetFeatures(encoder_depth=encoder_depth, decoder_channels=decoder_channels, backbone=backbone)\n",
        "        C = sum(decoder_channels[:encoder_depth])\n",
        "        # independent feature refinement\n",
        "        # concat features\n",
        "        # in:   B x C     x H//4 x W//4\n",
        "        # out:  B x C_Cat x H//4 x W//4\n",
        "        self.left_conv_cat = BasicBlock(inplanes=C, planes=12)\n",
        "        self.right_conv_cat = BasicBlock(inplanes=C, planes=12)\n",
        "        # correlation features\n",
        "        # in:   B x C     x H//4 x W//4\n",
        "        # out:  B x C_Cor x H//4 x W//4\n",
        "        self.left_conv_cor = BasicBlock(inplanes=C, planes=160)\n",
        "        self.right_conv_cor = BasicBlock(inplanes=C, planes=160)\n",
        "        # concat cost volume\n",
        "        # in: 2 of B x   C_Cat           x H//4 x W//4\n",
        "        # out:     B x 2*C_Cat x Disp//4 x H//4 x W//4\n",
        "        self.build_cost = CostBuilder(max_disp=max_disp)\n",
        "        # group-wise cost volume\n",
        "        # in: 2 of B x C_Cor              x H//4 x W//4\n",
        "        # out:     B x N_Groups x Disp//4 x H//4 x W//4\n",
        "        self.build_gwc = GWCBuilder(max_disp=max_disp, num_groups=40)\n",
        "        # 3d convs\n",
        "        # in:  B x N_Groups + 2*C_Cat x Disp//4 x H//4 x W//4\n",
        "        # out: B x 1                  x Disp//4 x H//4 x W//4\n",
        "        C3d = 64\n",
        "        self.stacked_hourglass = StackedHourglass(C3d)\n",
        "        # regressor\n",
        "        # in:  B x 1 x Disp//4 x H//4 x W//4\n",
        "        # out: B x 1           x H    x W\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=4),\n",
        "            Squeeze(dim=1),\n",
        "            torch.nn.Softmax(dim=1),\n",
        "            Expectation(max_disp)\n",
        "            )\n",
        "        \n",
        "    def forward(self, img_left, img_right):\n",
        "        # type: (Tensor, Tensor) -> Tensor\n",
        "        _, _, H, W = img_left.shape\n",
        "        feat_left, feat_right = self.features(img_left), self.features(img_right)\n",
        "        feat_left_cor, feat_right_cor = self.left_conv_cor(feat_left), self.right_conv_cor(feat_right)\n",
        "        feat_left_cat, feat_right_cat = self.left_conv_cat(feat_left), self.right_conv_cat(feat_right)\n",
        "        cost_gwc = self.build_gwc(feat_left_cor, feat_right_cor)\n",
        "        cost_cat = self.build_cost(feat_left_cat, feat_right_cat)\n",
        "        cost_volume = torch.cat([cost_gwc, cost_cat], dim=1)\n",
        "        if self.training:\n",
        "            (c1, c2, c3) = self.stacked_hourglass(cost_volume)\n",
        "            return (self.regressor(c1), self.regressor(c2), self.regressor(c3))\n",
        "        else:\n",
        "            c3 = self.stacked_hourglass(cost_volume)\n",
        "            return self.regressor(c3)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4unCkvmBPWFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b495de2-7cd8-4b35-84d4-48f04c6a2045"
      },
      "source": [
        "import time\n",
        "model = CustomModel(encoder_depth=3, backbone=\"mobilenet_v2\").cuda()\n",
        "with torch.no_grad():\n",
        "  with autocast():\n",
        "    features = model.features.encoder(batch_a)\n",
        "    #print('feats:')\n",
        "    #print(model.features.encoder._out_channels)\n",
        "    # for map in features:\n",
        "    #   print(map.shape)\n",
        "    start = time.time()\n",
        "    disparity_map = model(batch_a, batch_b)\n",
        "    torch.cuda.current_stream().synchronize()\n",
        "    print('Took', time.time()-start, 'seconds.')\n",
        "#print()\n",
        "print(type(disparity_map) is tuple)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 0.391345739364624 seconds.\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRl35gaBjg5N"
      },
      "source": [
        "### Image sizes in KITTI valid set\n",
        "\n",
        "torch.Size([1, 374, 1238])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 374, 1238])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 370, 1224])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 374, 1238])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 374, 1238])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 376, 1241])\n",
        "torch.Size([1, 375, 1242])\n",
        "torch.Size([1, 375, 1242])\n",
        "\n",
        " 352 or  384\n",
        " \n",
        "1216 or 1248"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL-No61nqdty"
      },
      "source": [
        "### Criterions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAvMW9TjjhbM"
      },
      "source": [
        "# class NonZeroWrapper(nn.Module):\n",
        "#     def __init__(self, criterion, eps=0):\n",
        "#         super(NonZeroWrapper, self).__init__()\n",
        "#         self.criterion = criterion\n",
        "#         self.eps = eps\n",
        "#     def forward(self, x, target):\n",
        "#         mask = target > self.eps\n",
        "#         loss = self.criterion(x, target)\n",
        "#         return loss[mask].mean()\n",
        "\n",
        "# class MaxDispWrapper(nn.Module):\n",
        "#     def __init__(self, criterion, max_disp=192):\n",
        "#         super(MaxDispWrapper, self).__init__()\n",
        "#         self.criterion = criterion\n",
        "#         self.max_disp = max_disp\n",
        "#     def forward(self, x, target):\n",
        "#         mask = target <= self.max_disp\n",
        "#         loss = self.criterion(x, target)\n",
        "#         return loss[mask].mean()\n",
        "\n",
        "# class PE(nn.Module):\n",
        "#     def __init__(self, threshold=3, reduction='mean'):\n",
        "#         super(PE, self).__init__()\n",
        "#         self.diff = nn.L1Loss(reduction='none')\n",
        "#         self.threshold = threshold\n",
        "#         self.reduction = reduction\n",
        "#     def forward(self, x, target):\n",
        "#         err = self.diff(x, target)\n",
        "#         mask_err = torch.ge(err, self.threshold)\n",
        "#         mask_ratio = torch.ge(err, 0.05*target)\n",
        "#         mask = torch.logical_and(mask_err, mask_ratio)\n",
        "#         loss = torch.where(mask, torch.ones_like(target), torch.zeros_like(target))\n",
        "#         if self.reduction == 'mean':\n",
        "#           return loss.mean()\n",
        "#         else:\n",
        "#           return loss\n",
        "\n",
        "# criterion_smoothl1 = torch.nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
        "# wrapped_criterion_smoothl1 = NonZeroWrapper(torch.nn.SmoothL1Loss(reduction='none', beta=1.0))\n",
        "# criterion_3PE = PE()\n",
        "# wrapped_criterion_3PE = NonZeroWrapper(PE(reduction='none'))\n",
        "# max_wrapped_criterion_smoothl1 = MaxDispWrapper(torch.nn.SmoothL1Loss(reduction='none', beta=1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDib8l2ACQxY"
      },
      "source": [
        "# B, C, H, W = 5, 3, 368, 1216\n",
        "# batch_a = 1*torch.randn(B, C, H, W).cuda()\n",
        "# batch_b = 500*torch.ones(B, C, H, W).cuda()\n",
        "# print(batch_a.shape, batch_b.shape)\n",
        "# print(torch.allclose(batch_a, batch_b), torch.allclose(batch_a, batch_a), torch.allclose(batch_b, batch_b))\n",
        "# mask = torch.randn(B, C, H, W).cuda() < 0.3\n",
        "# batch_b[mask] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbEaErmYAdsU"
      },
      "source": [
        "# loss = wrapped_criterion_smoothl1(batch_a, batch_b)\n",
        "# print(loss)\n",
        "# loss_max_disp = max_wrapped_criterion_smoothl1(batch_a, batch_b)\n",
        "# print(loss_max_disp)\n",
        "# loss_nw = criterion_smoothl1(batch_a, batch_b)\n",
        "# print(loss_nw)\n",
        "# loss_3pe = criterion_3PE(batch_a, batch_b)\n",
        "# print(loss_3pe)\n",
        "# loss_3pe_wrapped = wrapped_criterion_3PE(batch_a, batch_b)\n",
        "# print(loss_3pe_wrapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkDXM5ZCqn6B"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmBnQi4qtyIW"
      },
      "source": [
        "# Error cmap for GwC Net is available: https://github.com/xy-guo/GwcNet/blob/master/utils/visualization.py\n",
        "\n",
        "# For disparity maps, use Turbo: https://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZDqQbqAq6KT"
      },
      "source": [
        "# # As per: https://gist.github.com/mikhailov-work/ee72ba4191942acecc03fe6da94fc73f\n",
        "# import matplotlib.cm\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# turbo_colormap_data = [[0.18995,0.07176,0.23217],[0.19483,0.08339,0.26149],[0.19956,0.09498,0.29024],[0.20415,0.10652,0.31844],[0.20860,0.11802,0.34607],[0.21291,0.12947,0.37314],[0.21708,0.14087,0.39964],[0.22111,0.15223,0.42558],[0.22500,0.16354,0.45096],[0.22875,0.17481,0.47578],[0.23236,0.18603,0.50004],[0.23582,0.19720,0.52373],[0.23915,0.20833,0.54686],[0.24234,0.21941,0.56942],[0.24539,0.23044,0.59142],[0.24830,0.24143,0.61286],[0.25107,0.25237,0.63374],[0.25369,0.26327,0.65406],[0.25618,0.27412,0.67381],[0.25853,0.28492,0.69300],[0.26074,0.29568,0.71162],[0.26280,0.30639,0.72968],[0.26473,0.31706,0.74718],[0.26652,0.32768,0.76412],[0.26816,0.33825,0.78050],[0.26967,0.34878,0.79631],[0.27103,0.35926,0.81156],[0.27226,0.36970,0.82624],[0.27334,0.38008,0.84037],[0.27429,0.39043,0.85393],[0.27509,0.40072,0.86692],[0.27576,0.41097,0.87936],[0.27628,0.42118,0.89123],[0.27667,0.43134,0.90254],[0.27691,0.44145,0.91328],[0.27701,0.45152,0.92347],[0.27698,0.46153,0.93309],[0.27680,0.47151,0.94214],[0.27648,0.48144,0.95064],[0.27603,0.49132,0.95857],[0.27543,0.50115,0.96594],[0.27469,0.51094,0.97275],[0.27381,0.52069,0.97899],[0.27273,0.53040,0.98461],[0.27106,0.54015,0.98930],[0.26878,0.54995,0.99303],[0.26592,0.55979,0.99583],[0.26252,0.56967,0.99773],[0.25862,0.57958,0.99876],[0.25425,0.58950,0.99896],[0.24946,0.59943,0.99835],[0.24427,0.60937,0.99697],[0.23874,0.61931,0.99485],[0.23288,0.62923,0.99202],[0.22676,0.63913,0.98851],[0.22039,0.64901,0.98436],[0.21382,0.65886,0.97959],[0.20708,0.66866,0.97423],[0.20021,0.67842,0.96833],[0.19326,0.68812,0.96190],[0.18625,0.69775,0.95498],[0.17923,0.70732,0.94761],[0.17223,0.71680,0.93981],[0.16529,0.72620,0.93161],[0.15844,0.73551,0.92305],[0.15173,0.74472,0.91416],[0.14519,0.75381,0.90496],[0.13886,0.76279,0.89550],[0.13278,0.77165,0.88580],[0.12698,0.78037,0.87590],[0.12151,0.78896,0.86581],[0.11639,0.79740,0.85559],[0.11167,0.80569,0.84525],[0.10738,0.81381,0.83484],[0.10357,0.82177,0.82437],[0.10026,0.82955,0.81389],[0.09750,0.83714,0.80342],[0.09532,0.84455,0.79299],[0.09377,0.85175,0.78264],[0.09287,0.85875,0.77240],[0.09267,0.86554,0.76230],[0.09320,0.87211,0.75237],[0.09451,0.87844,0.74265],[0.09662,0.88454,0.73316],[0.09958,0.89040,0.72393],[0.10342,0.89600,0.71500],[0.10815,0.90142,0.70599],[0.11374,0.90673,0.69651],[0.12014,0.91193,0.68660],[0.12733,0.91701,0.67627],[0.13526,0.92197,0.66556],[0.14391,0.92680,0.65448],[0.15323,0.93151,0.64308],[0.16319,0.93609,0.63137],[0.17377,0.94053,0.61938],[0.18491,0.94484,0.60713],[0.19659,0.94901,0.59466],[0.20877,0.95304,0.58199],[0.22142,0.95692,0.56914],[0.23449,0.96065,0.55614],[0.24797,0.96423,0.54303],[0.26180,0.96765,0.52981],[0.27597,0.97092,0.51653],[0.29042,0.97403,0.50321],[0.30513,0.97697,0.48987],[0.32006,0.97974,0.47654],[0.33517,0.98234,0.46325],[0.35043,0.98477,0.45002],[0.36581,0.98702,0.43688],[0.38127,0.98909,0.42386],[0.39678,0.99098,0.41098],[0.41229,0.99268,0.39826],[0.42778,0.99419,0.38575],[0.44321,0.99551,0.37345],[0.45854,0.99663,0.36140],[0.47375,0.99755,0.34963],[0.48879,0.99828,0.33816],[0.50362,0.99879,0.32701],[0.51822,0.99910,0.31622],[0.53255,0.99919,0.30581],[0.54658,0.99907,0.29581],[0.56026,0.99873,0.28623],[0.57357,0.99817,0.27712],[0.58646,0.99739,0.26849],[0.59891,0.99638,0.26038],[0.61088,0.99514,0.25280],[0.62233,0.99366,0.24579],[0.63323,0.99195,0.23937],[0.64362,0.98999,0.23356],[0.65394,0.98775,0.22835],[0.66428,0.98524,0.22370],[0.67462,0.98246,0.21960],[0.68494,0.97941,0.21602],[0.69525,0.97610,0.21294],[0.70553,0.97255,0.21032],[0.71577,0.96875,0.20815],[0.72596,0.96470,0.20640],[0.73610,0.96043,0.20504],[0.74617,0.95593,0.20406],[0.75617,0.95121,0.20343],[0.76608,0.94627,0.20311],[0.77591,0.94113,0.20310],[0.78563,0.93579,0.20336],[0.79524,0.93025,0.20386],[0.80473,0.92452,0.20459],[0.81410,0.91861,0.20552],[0.82333,0.91253,0.20663],[0.83241,0.90627,0.20788],[0.84133,0.89986,0.20926],[0.85010,0.89328,0.21074],[0.85868,0.88655,0.21230],[0.86709,0.87968,0.21391],[0.87530,0.87267,0.21555],[0.88331,0.86553,0.21719],[0.89112,0.85826,0.21880],[0.89870,0.85087,0.22038],[0.90605,0.84337,0.22188],[0.91317,0.83576,0.22328],[0.92004,0.82806,0.22456],[0.92666,0.82025,0.22570],[0.93301,0.81236,0.22667],[0.93909,0.80439,0.22744],[0.94489,0.79634,0.22800],[0.95039,0.78823,0.22831],[0.95560,0.78005,0.22836],[0.96049,0.77181,0.22811],[0.96507,0.76352,0.22754],[0.96931,0.75519,0.22663],[0.97323,0.74682,0.22536],[0.97679,0.73842,0.22369],[0.98000,0.73000,0.22161],[0.98289,0.72140,0.21918],[0.98549,0.71250,0.21650],[0.98781,0.70330,0.21358],[0.98986,0.69382,0.21043],[0.99163,0.68408,0.20706],[0.99314,0.67408,0.20348],[0.99438,0.66386,0.19971],[0.99535,0.65341,0.19577],[0.99607,0.64277,0.19165],[0.99654,0.63193,0.18738],[0.99675,0.62093,0.18297],[0.99672,0.60977,0.17842],[0.99644,0.59846,0.17376],[0.99593,0.58703,0.16899],[0.99517,0.57549,0.16412],[0.99419,0.56386,0.15918],[0.99297,0.55214,0.15417],[0.99153,0.54036,0.14910],[0.98987,0.52854,0.14398],[0.98799,0.51667,0.13883],[0.98590,0.50479,0.13367],[0.98360,0.49291,0.12849],[0.98108,0.48104,0.12332],[0.97837,0.46920,0.11817],[0.97545,0.45740,0.11305],[0.97234,0.44565,0.10797],[0.96904,0.43399,0.10294],[0.96555,0.42241,0.09798],[0.96187,0.41093,0.09310],[0.95801,0.39958,0.08831],[0.95398,0.38836,0.08362],[0.94977,0.37729,0.07905],[0.94538,0.36638,0.07461],[0.94084,0.35566,0.07031],[0.93612,0.34513,0.06616],[0.93125,0.33482,0.06218],[0.92623,0.32473,0.05837],[0.92105,0.31489,0.05475],[0.91572,0.30530,0.05134],[0.91024,0.29599,0.04814],[0.90463,0.28696,0.04516],[0.89888,0.27824,0.04243],[0.89298,0.26981,0.03993],[0.88691,0.26152,0.03753],[0.88066,0.25334,0.03521],[0.87422,0.24526,0.03297],[0.86760,0.23730,0.03082],[0.86079,0.22945,0.02875],[0.85380,0.22170,0.02677],[0.84662,0.21407,0.02487],[0.83926,0.20654,0.02305],[0.83172,0.19912,0.02131],[0.82399,0.19182,0.01966],[0.81608,0.18462,0.01809],[0.80799,0.17753,0.01660],[0.79971,0.17055,0.01520],[0.79125,0.16368,0.01387],[0.78260,0.15693,0.01264],[0.77377,0.15028,0.01148],[0.76476,0.14374,0.01041],[0.75556,0.13731,0.00942],[0.74617,0.13098,0.00851],[0.73661,0.12477,0.00769],[0.72686,0.11867,0.00695],[0.71692,0.11268,0.00629],[0.70680,0.10680,0.00571],[0.69650,0.10102,0.00522],[0.68602,0.09536,0.00481],[0.67535,0.08980,0.00449],[0.66449,0.08436,0.00424],[0.65345,0.07902,0.00408],[0.64223,0.07380,0.00401],[0.63082,0.06868,0.00401],[0.61923,0.06367,0.00410],[0.60746,0.05878,0.00427],[0.59550,0.05399,0.00453],[0.58336,0.04931,0.00486],[0.57103,0.04474,0.00529],[0.55852,0.04028,0.00579],[0.54583,0.03593,0.00638],[0.53295,0.03169,0.00705],[0.51989,0.02756,0.00780],[0.50664,0.02354,0.00863],[0.49321,0.01963,0.00955],[0.47960,0.01583,0.01055]]\n",
        "# matplotlib.cm.register_cmap('turbo', cmap=ListedColormap(turbo_colormap_data))\n",
        "\n",
        "\n",
        "\n",
        "# batch_disp = torch.tile(torch.arange(192).unsqueeze(1), (1,192)).float()\n",
        "# batch_disp[100:160, 20:60] = 175\n",
        "# batch_disp[25:85, 130:170] = 17\n",
        "# print(batch_disp.shape)\n",
        "\n",
        "# plt.imshow(batch_disp, cmap='turbo', vmin=0, vmax=192)\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hpBUUtlrcVl"
      },
      "source": [
        "# # https://matplotlib.org/cmocean/\n",
        "# import cmocean\n",
        "\n",
        "# batch_diff = torch.tile(torch.arange(200).unsqueeze(1), (1,200)).float()\n",
        "# batch_diff[100:160, 20:60] = 200\n",
        "# batch_diff[25:85, 130:170] = 0\n",
        "# batch_diff -= 100\n",
        "# batch_diff = torch.abs(batch_diff)\n",
        "# print(batch_diff.shape)\n",
        "\n",
        "# plt.imshow(batch_diff, cmap='Spectral_r', vmin=0, vmax=90)\n",
        "# plt.title('spectral_r')\n",
        "# plt.colorbar()\n",
        "# plt.show()\n",
        "\n",
        "# plt.imshow(batch_diff, cmap='cmo.thermal', vmin=0, vmax=90)\n",
        "# plt.title('thermal')\n",
        "# plt.colorbar()\n",
        "# plt.show()\n",
        "\n",
        "# # Clipped colormap might be interesting for errors\n",
        "# newcmap = cmocean.tools.crop_by_percent(cmocean.cm.oxy_r, 20, which='min', N=None)\n",
        "\n",
        "# plt.imshow(batch_diff, cmap=newcmap, vmin=0, vmax=90)\n",
        "# plt.title('oxy')\n",
        "# plt.colorbar()\n",
        "# plt.show()\n",
        "\n",
        "# # Also try: https://stackoverflow.com/questions/16400241/how-to-redefine-a-color-for-a-specific-value-in-a-matplotlib-colormap for masked output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN8e0Tkrzn7a"
      },
      "source": [
        "# import copy\n",
        "# # make a function to plot disparity maps\n",
        "# # make a function to plot disparity map errors\n",
        "# # both should be able to write onto an ax or produce an individual plot\n",
        "# # disparity map should have an option to highlight invalid values\n",
        "# # disparity map errors should have two color map modes and an option to exclude incomplete groundtruth\n",
        "\n",
        "\n",
        "\n",
        "# def show_disparity(disparity, ax=None, vmin=0, vmax=192, remove_invalid=True, title='Disparity Map'):\n",
        "#     disparity = np.squeeze(disparity)\n",
        "#     turbo_colormap_data = [[0.18995,0.07176,0.23217],[0.19483,0.08339,0.26149],[0.19956,0.09498,0.29024],[0.20415,0.10652,0.31844],[0.20860,0.11802,0.34607],[0.21291,0.12947,0.37314],[0.21708,0.14087,0.39964],[0.22111,0.15223,0.42558],[0.22500,0.16354,0.45096],[0.22875,0.17481,0.47578],[0.23236,0.18603,0.50004],[0.23582,0.19720,0.52373],[0.23915,0.20833,0.54686],[0.24234,0.21941,0.56942],[0.24539,0.23044,0.59142],[0.24830,0.24143,0.61286],[0.25107,0.25237,0.63374],[0.25369,0.26327,0.65406],[0.25618,0.27412,0.67381],[0.25853,0.28492,0.69300],[0.26074,0.29568,0.71162],[0.26280,0.30639,0.72968],[0.26473,0.31706,0.74718],[0.26652,0.32768,0.76412],[0.26816,0.33825,0.78050],[0.26967,0.34878,0.79631],[0.27103,0.35926,0.81156],[0.27226,0.36970,0.82624],[0.27334,0.38008,0.84037],[0.27429,0.39043,0.85393],[0.27509,0.40072,0.86692],[0.27576,0.41097,0.87936],[0.27628,0.42118,0.89123],[0.27667,0.43134,0.90254],[0.27691,0.44145,0.91328],[0.27701,0.45152,0.92347],[0.27698,0.46153,0.93309],[0.27680,0.47151,0.94214],[0.27648,0.48144,0.95064],[0.27603,0.49132,0.95857],[0.27543,0.50115,0.96594],[0.27469,0.51094,0.97275],[0.27381,0.52069,0.97899],[0.27273,0.53040,0.98461],[0.27106,0.54015,0.98930],[0.26878,0.54995,0.99303],[0.26592,0.55979,0.99583],[0.26252,0.56967,0.99773],[0.25862,0.57958,0.99876],[0.25425,0.58950,0.99896],[0.24946,0.59943,0.99835],[0.24427,0.60937,0.99697],[0.23874,0.61931,0.99485],[0.23288,0.62923,0.99202],[0.22676,0.63913,0.98851],[0.22039,0.64901,0.98436],[0.21382,0.65886,0.97959],[0.20708,0.66866,0.97423],[0.20021,0.67842,0.96833],[0.19326,0.68812,0.96190],[0.18625,0.69775,0.95498],[0.17923,0.70732,0.94761],[0.17223,0.71680,0.93981],[0.16529,0.72620,0.93161],[0.15844,0.73551,0.92305],[0.15173,0.74472,0.91416],[0.14519,0.75381,0.90496],[0.13886,0.76279,0.89550],[0.13278,0.77165,0.88580],[0.12698,0.78037,0.87590],[0.12151,0.78896,0.86581],[0.11639,0.79740,0.85559],[0.11167,0.80569,0.84525],[0.10738,0.81381,0.83484],[0.10357,0.82177,0.82437],[0.10026,0.82955,0.81389],[0.09750,0.83714,0.80342],[0.09532,0.84455,0.79299],[0.09377,0.85175,0.78264],[0.09287,0.85875,0.77240],[0.09267,0.86554,0.76230],[0.09320,0.87211,0.75237],[0.09451,0.87844,0.74265],[0.09662,0.88454,0.73316],[0.09958,0.89040,0.72393],[0.10342,0.89600,0.71500],[0.10815,0.90142,0.70599],[0.11374,0.90673,0.69651],[0.12014,0.91193,0.68660],[0.12733,0.91701,0.67627],[0.13526,0.92197,0.66556],[0.14391,0.92680,0.65448],[0.15323,0.93151,0.64308],[0.16319,0.93609,0.63137],[0.17377,0.94053,0.61938],[0.18491,0.94484,0.60713],[0.19659,0.94901,0.59466],[0.20877,0.95304,0.58199],[0.22142,0.95692,0.56914],[0.23449,0.96065,0.55614],[0.24797,0.96423,0.54303],[0.26180,0.96765,0.52981],[0.27597,0.97092,0.51653],[0.29042,0.97403,0.50321],[0.30513,0.97697,0.48987],[0.32006,0.97974,0.47654],[0.33517,0.98234,0.46325],[0.35043,0.98477,0.45002],[0.36581,0.98702,0.43688],[0.38127,0.98909,0.42386],[0.39678,0.99098,0.41098],[0.41229,0.99268,0.39826],[0.42778,0.99419,0.38575],[0.44321,0.99551,0.37345],[0.45854,0.99663,0.36140],[0.47375,0.99755,0.34963],[0.48879,0.99828,0.33816],[0.50362,0.99879,0.32701],[0.51822,0.99910,0.31622],[0.53255,0.99919,0.30581],[0.54658,0.99907,0.29581],[0.56026,0.99873,0.28623],[0.57357,0.99817,0.27712],[0.58646,0.99739,0.26849],[0.59891,0.99638,0.26038],[0.61088,0.99514,0.25280],[0.62233,0.99366,0.24579],[0.63323,0.99195,0.23937],[0.64362,0.98999,0.23356],[0.65394,0.98775,0.22835],[0.66428,0.98524,0.22370],[0.67462,0.98246,0.21960],[0.68494,0.97941,0.21602],[0.69525,0.97610,0.21294],[0.70553,0.97255,0.21032],[0.71577,0.96875,0.20815],[0.72596,0.96470,0.20640],[0.73610,0.96043,0.20504],[0.74617,0.95593,0.20406],[0.75617,0.95121,0.20343],[0.76608,0.94627,0.20311],[0.77591,0.94113,0.20310],[0.78563,0.93579,0.20336],[0.79524,0.93025,0.20386],[0.80473,0.92452,0.20459],[0.81410,0.91861,0.20552],[0.82333,0.91253,0.20663],[0.83241,0.90627,0.20788],[0.84133,0.89986,0.20926],[0.85010,0.89328,0.21074],[0.85868,0.88655,0.21230],[0.86709,0.87968,0.21391],[0.87530,0.87267,0.21555],[0.88331,0.86553,0.21719],[0.89112,0.85826,0.21880],[0.89870,0.85087,0.22038],[0.90605,0.84337,0.22188],[0.91317,0.83576,0.22328],[0.92004,0.82806,0.22456],[0.92666,0.82025,0.22570],[0.93301,0.81236,0.22667],[0.93909,0.80439,0.22744],[0.94489,0.79634,0.22800],[0.95039,0.78823,0.22831],[0.95560,0.78005,0.22836],[0.96049,0.77181,0.22811],[0.96507,0.76352,0.22754],[0.96931,0.75519,0.22663],[0.97323,0.74682,0.22536],[0.97679,0.73842,0.22369],[0.98000,0.73000,0.22161],[0.98289,0.72140,0.21918],[0.98549,0.71250,0.21650],[0.98781,0.70330,0.21358],[0.98986,0.69382,0.21043],[0.99163,0.68408,0.20706],[0.99314,0.67408,0.20348],[0.99438,0.66386,0.19971],[0.99535,0.65341,0.19577],[0.99607,0.64277,0.19165],[0.99654,0.63193,0.18738],[0.99675,0.62093,0.18297],[0.99672,0.60977,0.17842],[0.99644,0.59846,0.17376],[0.99593,0.58703,0.16899],[0.99517,0.57549,0.16412],[0.99419,0.56386,0.15918],[0.99297,0.55214,0.15417],[0.99153,0.54036,0.14910],[0.98987,0.52854,0.14398],[0.98799,0.51667,0.13883],[0.98590,0.50479,0.13367],[0.98360,0.49291,0.12849],[0.98108,0.48104,0.12332],[0.97837,0.46920,0.11817],[0.97545,0.45740,0.11305],[0.97234,0.44565,0.10797],[0.96904,0.43399,0.10294],[0.96555,0.42241,0.09798],[0.96187,0.41093,0.09310],[0.95801,0.39958,0.08831],[0.95398,0.38836,0.08362],[0.94977,0.37729,0.07905],[0.94538,0.36638,0.07461],[0.94084,0.35566,0.07031],[0.93612,0.34513,0.06616],[0.93125,0.33482,0.06218],[0.92623,0.32473,0.05837],[0.92105,0.31489,0.05475],[0.91572,0.30530,0.05134],[0.91024,0.29599,0.04814],[0.90463,0.28696,0.04516],[0.89888,0.27824,0.04243],[0.89298,0.26981,0.03993],[0.88691,0.26152,0.03753],[0.88066,0.25334,0.03521],[0.87422,0.24526,0.03297],[0.86760,0.23730,0.03082],[0.86079,0.22945,0.02875],[0.85380,0.22170,0.02677],[0.84662,0.21407,0.02487],[0.83926,0.20654,0.02305],[0.83172,0.19912,0.02131],[0.82399,0.19182,0.01966],[0.81608,0.18462,0.01809],[0.80799,0.17753,0.01660],[0.79971,0.17055,0.01520],[0.79125,0.16368,0.01387],[0.78260,0.15693,0.01264],[0.77377,0.15028,0.01148],[0.76476,0.14374,0.01041],[0.75556,0.13731,0.00942],[0.74617,0.13098,0.00851],[0.73661,0.12477,0.00769],[0.72686,0.11867,0.00695],[0.71692,0.11268,0.00629],[0.70680,0.10680,0.00571],[0.69650,0.10102,0.00522],[0.68602,0.09536,0.00481],[0.67535,0.08980,0.00449],[0.66449,0.08436,0.00424],[0.65345,0.07902,0.00408],[0.64223,0.07380,0.00401],[0.63082,0.06868,0.00401],[0.61923,0.06367,0.00410],[0.60746,0.05878,0.00427],[0.59550,0.05399,0.00453],[0.58336,0.04931,0.00486],[0.57103,0.04474,0.00529],[0.55852,0.04028,0.00579],[0.54583,0.03593,0.00638],[0.53295,0.03169,0.00705],[0.51989,0.02756,0.00780],[0.50664,0.02354,0.00863],[0.49321,0.01963,0.00955],[0.47960,0.01583,0.01055]]\n",
        "#     cmap = ListedColormap(turbo_colormap_data)\n",
        "#     if remove_invalid:\n",
        "#         cmap.set_under('k')\n",
        "#     if ax:\n",
        "#         ln = ax.imshow(disparity, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "#         ln = ax.set_title(title)\n",
        "#         ln = ax.axis('off')\n",
        "#         return ln\n",
        "#     else:\n",
        "#         plt.imshow(disparity, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "#         plt.title(title)\n",
        "#         plt.axis('off')\n",
        "#         plt.show()\n",
        "#     return None\n",
        "\n",
        "# def show_error(error, ax=None, vmin=0, vmax=100, remove_invalid=True, title='Absolute Error', highlight_large=False, diverging=False):\n",
        "#     error = np.squeeze(error)\n",
        "#     cmap = matplotlib.cm.get_cmap('Spectral_r') if diverging else cmocean.cm.thermal\n",
        "#     if highlight_large:\n",
        "#         cmap = cmocean.tools.crop_by_percent(cmocean.cm.oxy_r, 20, which='min', N=None)\n",
        "#     cmap = copy.deepcopy(cmap)\n",
        "#     if remove_invalid:\n",
        "#         cmap.set_under('k')\n",
        "#     if ax:\n",
        "#         ln = ax.imshow(error, cmap=cmap) if highlight_large else ax.imshow(error, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "#         ln = ax.set_title(title)\n",
        "#         ln = ax.axis('off')\n",
        "#         return ln\n",
        "#     else:\n",
        "#         plt.imshow(error, cmap=cmap) if highlight_large else plt.imshow(error, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "#         plt.title(title)\n",
        "#         plt.axis('off')\n",
        "#         plt.show()\n",
        "#     return None\n",
        "\n",
        "# def show_image(image, ax=None, title='Frame'):\n",
        "#     image = image.permute(1,2,0)\n",
        "#     if ax:\n",
        "#         ln = ax.imshow(image)\n",
        "#         ln = ax.set_title(title)\n",
        "#         ln = ax.axis('off')\n",
        "#         return ln\n",
        "#     else:\n",
        "#         plt.imshow(image)\n",
        "#         plt.title(title)\n",
        "#         plt.axis('off')\n",
        "#         plt.show()\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u5c5M5Kaf9B"
      },
      "source": [
        "# batch_imgs = torch.rand((3,256,512))\n",
        "\n",
        "# batch_disp_masked = batch_disp.clone()\n",
        "# mask = torch.rand_like(batch_disp_masked) < 0.\n",
        "# batch_disp_masked[mask] = -1\n",
        "\n",
        "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "# show_disparity(batch_disp_masked, ax1)\n",
        "# show_error(batch_diff, ax2)\n",
        "# show_image(batch_imgs, ax3)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# show_disparity(batch_disp_masked)\n",
        "# show_error(batch_diff)\n",
        "# show_image(batch_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaCUPRnYCvUF"
      },
      "source": [
        "def conv3d3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv3d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv3d1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv3d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "    \n",
        "class BasicBlock3D(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride = 1, norm_layer = None):\n",
        "        super(BasicBlock3D, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm3d\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3d3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3d3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = None if (stride == 1 and inplanes == planes) else nn.Sequential(conv3d1x1(inplanes, planes, stride), norm_layer(planes))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # type: (Tensor) -> Tensor\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# in:  B x N_Groups + 2*C_Cat x Disp//4 x H//4 x W//4\n",
        "# out: B x 1                  x Disp//4 x H//4 x W//4\n",
        "\n",
        "C3d = 64\n",
        "test_conv_3d = nn.Sequential(\n",
        "    BasicBlock3D(inplanes=C3d,    planes=C3d//2),\n",
        "    BasicBlock3D(inplanes=C3d//2, planes=C3d//4),\n",
        "    BasicBlock3D(inplanes=C3d//4, planes=C3d//8),\n",
        "    BasicBlock3D(inplanes=C3d//8, planes=1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay8bCSzFJJVk"
      },
      "source": [
        "test_conv_3d = test_conv_3d.cuda()\n",
        "tmp_batch = torch.rand((2,64, 192//4, 256//4, 512//4)).cuda()\n",
        "tmp_out = test_conv_3d(tmp_batch)\n",
        "print(tmp_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0JtT9ncJhTy"
      },
      "source": [
        "# Notes: Apply relu to sum\n",
        "# batchnorm after every conv3d\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}